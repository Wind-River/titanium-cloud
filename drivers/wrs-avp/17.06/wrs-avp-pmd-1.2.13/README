BSD LICENSE

Copyright(c) 2010-2013 Intel Corporation. All rights reserved.
Copyright(c) 2013-2017, Wind River Systems, Inc. All rights reserved.
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions
are met:

  * Redistributions of source code must retain the above copyright
    notice, this list of conditions and the following disclaimer.
  * Redistributions in binary form must reproduce the above copyright
    notice, this list of conditions and the following disclaimer in
    the documentation and/or other materials provided with the
    distribution.
  * Neither the name of Intel Corporation nor the names of its
    contributors may be used to endorse or promote products derived
    from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-----------------------------------------------------------------------

DESCRIPTION
===========
Titanium Cloud AVP virtual NIC is a shared memory based high performance
networking device.  Its potential maximum throughput is higher than other
standard virtual NIC devices (e.g., e1000, virtio).  This package provides the
an Intel DPDK compatible Poll Mode Driver(PMD).  It can be compiled as a
component of an Intel DPDK distribution.


REQUIREMENTS
============
    Compilation:
        Intel DPDK == v2.0.x, v2.2.x, v16.04, v16.11, v17.02
        64-bit Linux Kernel
           version >= 3.2
           loadable module support
           UIO device support
           hugepage support
        gcc compiler

    VM Runtime:
        AVP type virtual NIC
        UIO kernel module


DELIVERABLE
===========
Titanium Cloud AVP Poll Mode Driver(PMD) is delivered as source with the
required DPDK make and configuration files in a compressed tarball such that
it can be compiled as a component of an Intel DPDK distribution.


COMPILE (RTE_VERSION >= 1.7.x)
=======
Extract the tarball contents to a working directory.

   ## 1. Extract
   ##    NOTE: The expected value of the RTE_SDK variable below is the final
   ##      install path of the DPDK distribution rather than the tar extracted
   ##      build directory.  The DPDK distribution is installed using the
   ##      "make install" command at the end of the build process; as the root
   ##      user.  Depending on your build environment this may be
   ##      /usr/local/share/dpdk or a similar path.
   export RTE_SDK=/path/to/installed/dpdk
   export RTE_TARGET=<name of dpdk target>
   mkdir -p /tmp/wrs; cd /tmp/wrs
   tar -xvf wrs-avp-pmd-1.2.13.tgz
   export WRS_SDK=${PWD}/wrs-avp-pmd-1.2.13

   ## 2. Compile
   ##
   cd ${WRS_SDK}/drivers/net/avp
   export WRS_PMD_SHARED_LIB=y
   make

   ## 3. Install to the shared library directory specific to your system
   ##    (e.g., /usr/local/lib)
   ##
   sudo cp ${WRS_SDK}/wrs/lib/libwrs_pmd_avp/build/lib/libwrs_pmd_avp.so /usr/local/lib


COMPATIBILITY
=============
Some kernel version do not export sysfs paths for NUMA information unless the
system has more than a single NUMA socket.  The DPDK software assumes that
theses paths are always present.  The DPDK software uses the following path to
determine the NUMA socket (YY) of a specific logical CPU (XX).

/sys/devices/system/cpu/cpuXX/nodeYY/

When the DPDK it is unable to make a determination using this path it falls
back to using the following path as an approximation.  Unfortunately, the
package id of a logical CPU in a VM instance may not provide an accurate guess
as to a valid NUMA socket id.  When this assumption fails this can lead an
DPDK application to attempt to allocate memory on a non-existent NUMA socket.

/sys/devices/system/cpu/cpu0/topology/physical_package_id

To avoid this shortcoming it is often better to simply assume that all logical
CPUs are part of a single NUMA socket.  The following patch file adapts the
DPDK software to simply assume a NUMA socket of "0" if the initial sysfs path
is not present.  This is not required on all distributions.  Only apply this
patch if your distribution is affected.  Currently, Wind River Linux 5 (WRL5)
is the only known distribution.  Check your distribution for the sysfs path
mentionned above before applying this patch. Note that there is a version
specific patch for the supported DPDK versions.

${WRS_SDK}/extras-1.5.0/0001-Removed-CPU-package-id-and-socket-id-assumption.patch


RUNTIME
=======
At present, the WRS AVP devices are displayed by the "lspci" utilities as
generic Red Hat memory devices.  Depending on your system configuration they
will be bound to different PCI device addresses.  For example,

root@localhost$ lspci
   ... ...
   00:06.0 RAM memory: Red Hat, Inc Device 1110
   00:07.0 RAM memory: Red Hat, Inc Device 1110

As is normal with standard DPDK applications, all devices to be bound to a
DPDK application must first be unbound from their current driver (if any) and
rebound to a UIO kernel device driver.  The "igb_uio" driver which ships with
the Intel DPDK is sufficient to bind to the AVP devices.  To do this the
"igb_uio" module must first be programmed with the AVP PCI vendor and device
identifiers.  For example,

   modprobe uio
   insmod ${RTE_SDK}/${RTE_TARGET}/kmod/igb_uio.ko
   echo "1af4 1110" > /sys/bus/pci/drivers/igb_uio/new_id

Then the devices must be explicitly unbound from their current driver and
rebound to the igb_uio driver.

   echo -n "0000:00:06.0" > /sys/bus/pci/devices/0000:00:06.0/driver/unbind'
   echo -n "0000:00:06.0" > /sys/bus/pci/drivers/igb_uio/bind'
   echo -n "0000:00:07.0" > /sys/bus/pci/devices/0000:00:07.0/driver/unbind'
   echo -n "0000:00:07.0" > /sys/bus/pci/drivers/igb_uio/bind'

Finally, the DPDK application can be launched by specifying the AVP PMD shared
library as an argument.  The location of the shared library must be in
LD_LIBRARY_PATH or configured in the /etc/ld.conf configuration file.

   testpmd -n 2 -c 0x7 -m 128 -d libwrs_pmd_avp.so -- -i


SUPPORTED DPDK APIs
=======================
The WRS AVP PMD is designed to coexist in the standard Intel DPDK software
hierarchy.  As such it supports the standard ethernet driver API.  That is,
the following list of API functions are supported.

    rte_eth_dev_configure
    rte_eth_rx_queue_setup
    rte_eth_tx_queue_setup
    rte_eth_dev_socket_id
    rte_eth_dev_start
    rte_eth_dev_stop
    rte_eth_dev_close
    rte_eth_promiscuous_enable
    rte_eth_promiscuous_disable
    rte_eth_promiscuous_get
    rte_eth_link_get
    rte_eth_link_get_nowait
    rte_eth_stats_get
    rte_eth_stats_reset
    rte_eth_macaddr_get
    rte_eth_dev_info_get
    rte_eth_rx_burst
    rte_eth_tx_burst


LIMITATIONS
=======================
The WRS AVP PMD module has the following limitations.

1.  The maximum number of queues are TX=8, RX=8
2.  The maximum number of MAC addresses is 1
3.  The MAC address cannot be modified
4.  The maximum receive packet length is 9238 bytes
5.  The maximum number of chained mbufs is 5


HARDWARE OFFLOAD FEATURES
=======================
The WRS AVP PMD supports the following hardware offload features.  Unless
stated otherwise, all other offload capabilities are not supported and should
not be specified.  Querying the device info using the rte_eth_dev_info_get()
API will report which offload capabilities are supported in the
'rx_offload_capa' and 'tx_offload_capa' attributes.

1.  VLAN insert and strip.

  The device will report that it supports both VLAN insertion and stripping
  (i.e., DEV_RX_OFFLOAD_VLAN_STRIP asserted in rx_offload_capa field, and
  DEV_TX_OFFLOAD_VLAN_INSERT asserted in tx_offload_capa field). The receive
  and transmit capabilities are reported as individual functions but are
  controlled as a combined feature using the rxmode.hw_vlan_strip attribute of
  the rte_eth_dev_configure() API parameter.

  Enabling this feature allows the guest and host to exchange VLAN tagging
  information in packet metadata rather than modifying packet headers to add
  and remove VLAN tagging information.  In many circumstances this capability
  reduces CPU cost associated to processing VLAN tagged packets at both the
  guest and host levels.
